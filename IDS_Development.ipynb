{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.core.internals.blocks import F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from zipfile import ZipFile\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "maUo2WBqdneE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade scikit-learn"
      ],
      "metadata": {
        "id": "YDMQc4HLl9KK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ed829cb-cd92-4f3b-f641-b7d19ae18ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import neighbors, datasets\n",
        "#from sklearn.inspection import DecisionBoundaryDisplay\n",
        "import sklearn.metrics\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import preprocessing\n",
        "label_encoder = preprocessing.LabelEncoder()"
      ],
      "metadata": {
        "id": "ZElHOcGcdq9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VwT0YMKUZuwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "475ae46c-00ea-4a32-958d-51a9c452bab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To confirm the drive is properly mounted, let's list to show outline and contents.\n",
        "!ls drive/MyDrive/"
      ],
      "metadata": {
        "id": "MyiINNxuaDYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset is a zip file, we have to extract the individual files using the ZipFile python library.\n",
        "with ZipFile(\"drive/MyDrive/CAN_dataset.zip\", \"r\") as zObject:\n",
        "    zObject.extractall()"
      ],
      "metadata": {
        "id": "L-KzklMXaPK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To confirm the zipfile was extract, we'll list the content of this notebote file path\n",
        "!ls"
      ],
      "metadata": {
        "id": "pe2eoAi0an5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f42a12db-9531-4b80-9cfa-bab3e80547ea"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CAN_attack_dataset2  drive  __MACOSX  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading of Dataset"
      ],
      "metadata": {
        "id": "UpIcxtYCjtRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since the dataset is made up of subdataset, we'll read them individually using the function below.\n",
        "folder_name = \"CAN_dataset\"\n",
        "\n",
        "def read_datasets(folder_name: str):\n",
        "    AttackFree= pd.read_csv(f'{folder_name}/Attack_free.csv')[0:136933]\n",
        "    Flooding= pd.read_csv(f'{folder_name}/Flooding.csv')[0:84999]\n",
        "    Fuzzy= pd.read_csv(f'{folder_name}/Fuzzy.csv')[0:40999]\n",
        "    Malfunction= pd.read_csv(f'{folder_name}/Malfunction.csv')[0:50999]\n",
        "\n",
        "# Then call the defined function.\n",
        "read_datasets(folder_name)"
      ],
      "metadata": {
        "id": "VgTvd5tCEN_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label Encoding"
      ],
      "metadata": {
        "id": "8WDR8CapjqoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attack-Free\n",
        "\"\"\"\n",
        "For each column (from index 1 to 15) in the AttackFree DataFrame below,\n",
        "the fit_transform method of the label_encoder object is applied to encode categorical labels into numerical values.\n",
        "This transformation is applied in place.\n",
        "\"\"\"\n",
        "\n",
        "AttackFree.iloc[:, 1]= label_encoder.fit_transform(AttackFree.iloc[:, 1])\n",
        "AttackFree.iloc[:, 2]= label_encoder.fit_transform(AttackFree.iloc[:, 2])\n",
        "AttackFree.iloc[:, 3]= label_encoder.fit_transform(AttackFree.iloc[:, 3])\n",
        "AttackFree.iloc[:, 4]= label_encoder.fit_transform(AttackFree.iloc[:, 4])\n",
        "AttackFree.iloc[:, 5]= label_encoder.fit_transform(AttackFree.iloc[:, 5])\n",
        "AttackFree.iloc[:, 6]= label_encoder.fit_transform(AttackFree.iloc[:, 6])\n",
        "AttackFree.iloc[:, 7]= label_encoder.fit_transform(AttackFree.iloc[:, 7])\n",
        "AttackFree.iloc[:, 8]= label_encoder.fit_transform(AttackFree.iloc[:, 8])\n",
        "AttackFree.iloc[:, 9]= label_encoder.fit_transform(AttackFree.iloc[:, 9])\n",
        "AttackFree.iloc[:, 10]= label_encoder.fit_transform(AttackFree.iloc[:, 10])\n",
        "AttackFree.iloc[:, 11]= label_encoder.fit_transform(AttackFree.iloc[:, 11])\n",
        "AttackFree.iloc[:, 12]= label_encoder.fit_transform(AttackFree.iloc[:, 12])\n",
        "AttackFree.iloc[:, 13]= label_encoder.fit_transform(AttackFree.iloc[:, 13])\n",
        "AttackFree.iloc[:, 14]= label_encoder.fit_transform(AttackFree.iloc[:, 14])\n",
        "AttackFree.iloc[:, 15]= label_encoder.fit_transform(AttackFree.iloc[:, 15])\n",
        "\n",
        "# Flooding\n",
        "\"\"\"\n",
        "Same thing in the AttackFree class was done below, in the Flooding, Fuzzy and Malfunction classes.\n",
        "\"\"\"\n",
        "Flooding.iloc[:, 1]= label_encoder.fit_transform(Flooding.iloc[:, 1])\n",
        "Flooding.iloc[:, 2]= label_encoder.fit_transform(Flooding.iloc[:, 2])\n",
        "Flooding.iloc[:, 3]= label_encoder.fit_transform(Flooding.iloc[:, 3])\n",
        "Flooding.iloc[:, 4]= label_encoder.fit_transform(Flooding.iloc[:, 4])\n",
        "Flooding.iloc[:, 5]= label_encoder.fit_transform(Flooding.iloc[:, 5])\n",
        "Flooding.iloc[:, 6]= label_encoder.fit_transform(Flooding.iloc[:, 6])\n",
        "Flooding.iloc[:, 7]= label_encoder.fit_transform(Flooding.iloc[:, 7])\n",
        "Flooding.iloc[:, 8]= label_encoder.fit_transform(Flooding.iloc[:, 8])\n",
        "Flooding.iloc[:, 9]= label_encoder.fit_transform(Flooding.iloc[:, 9])\n",
        "Flooding.iloc[:, 10]= label_encoder.fit_transform(Flooding.iloc[:, 10])\n",
        "Flooding.iloc[:, 11]= label_encoder.fit_transform(Flooding.iloc[:, 11])\n",
        "Flooding.iloc[:, 12]= label_encoder.fit_transform(Flooding.iloc[:, 12])\n",
        "Flooding.iloc[:, 13]= label_encoder.fit_transform(Flooding.iloc[:, 13])\n",
        "Flooding.iloc[:, 14]= label_encoder.fit_transform(Flooding.iloc[:, 14])\n",
        "Flooding.iloc[:, 15]= label_encoder.fit_transform(Flooding.iloc[:, 15])\n",
        "\n",
        "# Fuzzy\n",
        "Fuzzy.iloc[:, 1]= label_encoder.fit_transform(Fuzzy.iloc[:, 1])\n",
        "Fuzzy.iloc[:, 2]= label_encoder.fit_transform(Fuzzy.iloc[:, 2])\n",
        "Fuzzy.iloc[:, 3]= label_encoder.fit_transform(Fuzzy.iloc[:, 3])\n",
        "Fuzzy.iloc[:, 4]= label_encoder.fit_transform(Fuzzy.iloc[:, 4])\n",
        "Fuzzy.iloc[:, 5]= label_encoder.fit_transform(Fuzzy.iloc[:, 5])\n",
        "Fuzzy.iloc[:, 6]= label_encoder.fit_transform(Fuzzy.iloc[:, 6])\n",
        "Fuzzy.iloc[:, 7]= label_encoder.fit_transform(Fuzzy.iloc[:, 7])\n",
        "Fuzzy.iloc[:, 8]= label_encoder.fit_transform(Fuzzy.iloc[:, 8])\n",
        "Fuzzy.iloc[:, 9]= label_encoder.fit_transform(Fuzzy.iloc[:, 9])\n",
        "Fuzzy.iloc[:, 10]= label_encoder.fit_transform(Fuzzy.iloc[:, 10])\n",
        "Fuzzy.iloc[:, 11]= label_encoder.fit_transform(Fuzzy.iloc[:, 11])\n",
        "Fuzzy.iloc[:, 12]= label_encoder.fit_transform(Fuzzy.iloc[:, 12])\n",
        "Fuzzy.iloc[:, 13]= label_encoder.fit_transform(Fuzzy.iloc[:, 13])\n",
        "Fuzzy.iloc[:, 14]= label_encoder.fit_transform(Fuzzy.iloc[:, 14])\n",
        "Fuzzy.iloc[:, 15]= label_encoder.fit_transform(Fuzzy.iloc[:, 15])\n",
        "\n",
        "# Malfunction\n",
        "Malfunction.iloc[:, 1]= label_encoder.fit_transform(Malfunction.iloc[:, 1])\n",
        "Malfunction.iloc[:, 2]= label_encoder.fit_transform(Malfunction.iloc[:, 2])\n",
        "Malfunction.iloc[:, 3]= label_encoder.fit_transform(Malfunction.iloc[:, 3])\n",
        "Malfunction.iloc[:, 4]= label_encoder.fit_transform(Malfunction.iloc[:, 4])\n",
        "Malfunction.iloc[:, 5]= label_encoder.fit_transform(Malfunction.iloc[:, 5])\n",
        "Malfunction.iloc[:, 6]= label_encoder.fit_transform(Malfunction.iloc[:, 6])\n",
        "Malfunction.iloc[:, 7]= label_encoder.fit_transform(Malfunction.iloc[:, 7])\n",
        "Malfunction.iloc[:, 8]= label_encoder.fit_transform(Malfunction.iloc[:, 8])\n",
        "Malfunction.iloc[:, 9]= label_encoder.fit_transform(Malfunction.iloc[:, 9])\n",
        "Malfunction.iloc[:, 10]= label_encoder.fit_transform(Malfunction.iloc[:, 10])\n",
        "Malfunction.iloc[:, 11]= label_encoder.fit_transform(Malfunction.iloc[:, 11])\n",
        "Malfunction.iloc[:, 12]= label_encoder.fit_transform(Malfunction.iloc[:, 12])\n",
        "Malfunction.iloc[:, 13]= label_encoder.fit_transform(Malfunction.iloc[:, 13])\n",
        "Malfunction.iloc[:, 14]= label_encoder.fit_transform(Malfunction.iloc[:, 14])\n",
        "Malfunction.iloc[:, 15]= label_encoder.fit_transform(Malfunction .iloc[:, 15])\n",
        "\n",
        "\"\"\"\n",
        "What was done in this stage is label encoding.\n",
        "Label encoding is a process of converting categorical data into numerical form.\n",
        "It assigns a unique integer to each category in the feature.\n",
        "This transformation is often used in machine learning algorithms that expect numerical input data.\n",
        "However, it's essential to note that label encoding might introduce an implicit ordinal relationship between categories,\n",
        "which may not exist in the data.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0P-GZa8SEVYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Labels for Each Class"
      ],
      "metadata": {
        "id": "rS29rL1BkCBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Four lists (label1, label2, label3, label4) are created below,\n",
        "each representing the labels for one of the four classes (Attack-Free, Flooding, Fuzzy, Malfunction).\n",
        "Each list is populated with the respective class label\n",
        "which repeated as many times as there are instances in the corresponding DataFrame.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "label1 = []\n",
        "for i in range(len(AttackFree)):\n",
        "  label1.append(1)\n",
        "\n",
        "label2 = []\n",
        "for i in range(len(Flooding)):\n",
        "  label2.append(2)\n",
        "\n",
        "label3 = []\n",
        "for i in range(len(Fuzzy)):\n",
        "  label3.append(3)\n",
        "\n",
        "label4 = []\n",
        "for i in range(len(Malfunction)):\n",
        "  label4.append(4)\n",
        "\n",
        "# The np.concatenate() function below is used\n",
        "# to concatenate the Attack-Free, Flooding, Fuzzy, and Malfunction DataFrames into a single Dataset array.\n",
        "\n",
        "Dataset = np.concatenate((AttackFree, Flooding, Fuzzy, Malfunction))\n",
        "label = np.concatenate((label1, label2, label3, label4))\n",
        "\n"
      ],
      "metadata": {
        "id": "9_A4P5d2EyEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Splitting"
      ],
      "metadata": {
        "id": "WNqeqS_AkoBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "X_Train, X_Test, Y_Train, Y_Test = train_test_split(Dataset,\n",
        "                                                    label,\n",
        "                                                    test_size = 0.25,\n",
        "                                                    random_state = 0)\n",
        "\n",
        "# It splits the data randomly, with 75% of the data used for training and 25% for testing."
      ],
      "metadata": {
        "id": "8Arq6cDLkm5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Scaling"
      ],
      "metadata": {
        "id": "LgaLW3N5kqzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scale(X_Train, X_Test):\n",
        "    sc_X = StandardScaler()\n",
        "    X_Train = sc_X.fit_transform(X_Train)\n",
        "    X_Test = sc_X.transform(X_Test)\n",
        "    # transform() is called on X_Test to scale the test set using the same scaling parameters learned from the training set.\n",
        "\n",
        "scale()\n",
        "# Call the scale funciton on the features"
      ],
      "metadata": {
        "id": "4sHFTAToklB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# So we can plot the confusion matrices in later blocks\n",
        "label = ['AttackFree', 'DoS', 'Fuzzy', 'Impersonation']"
      ],
      "metadata": {
        "id": "wRTNwPE8nDQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train SVM Classifier"
      ],
      "metadata": {
        "id": "PviPQt8Tky0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM classifier initialization\n",
        "classifier_svm = SVC(kernel = 'rbf',\n",
        "                     gamma='scale',\n",
        "                     random_state = 0,\n",
        "                     max_iter=-1,\n",
        "                     tol=0.0001,\n",
        "                     degree=3,\n",
        "                     verbose=False,\n",
        "                     class_weight=None,\n",
        "                     C=100.0)\n",
        "\n",
        "start_time_1 = datetime.now()\n",
        "\n",
        "classifier_svm.fit(X_Train, Y_Train)\n",
        "\n",
        "end_time_1 = datetime.now()\n",
        "print('Training Duration: {}'.format(end_time_1 - start_time_1))\n",
        "\n",
        "# SVM Testing\n",
        "start_time_2 = datetime.now()\n",
        "\n",
        "# Predicting the test set results\n",
        "Y_Pred_svm = classifier_svm.predict(X_Test)\n",
        "\n",
        "end_time_2 = datetime.now()\n",
        "print('Testing Duration: {}'.format(end_time_2 - start_time_2))\n",
        "\n",
        "# Here, we train an SVM classifier with an RBF kernel on the training data and evaluates its performance on the test data."
      ],
      "metadata": {
        "id": "_DzEja7OmXQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "380aa168-0781-4595-a5b3-bae05e54fba8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Duration: 0:13:23.291517\n",
            "Testing Duration: 0:01:27.380223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM Confusion Matrix"
      ],
      "metadata": {
        "id": "sjR0MHx6lPbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making the Confusion Matrix :SVM\n",
        "\"\"\"\n",
        "We'll calculate the confusion matrix for the true labels (Y_Test) and\n",
        "predicted labels (Y_Pred_svm) obtained from the SVM classifier.\n",
        "\"\"\"\n",
        "\n",
        "cm_svm = confusion_matrix(Y_Test, Y_Pred_svm)\n",
        "cm_svm\n",
        "\n",
        "font = {'family' : 'normal',\n",
        "        'size'   : 14}\n",
        "\n",
        "plt.rc('font', **font)\n",
        "\n",
        "df_cm_svm = pd.DataFrame(cm_svm, index = [i for i in label],\n",
        "                  columns = [i for i in label])\n",
        "\n",
        "plt.figure(figsize = (7,5))\n",
        "sn.heatmap(df_cm_svm, annot=True, fmt='.0f', cmap='Blues')\n",
        "plt.ylabel('True Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nR--MLJNFBW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM Evaluation"
      ],
      "metadata": {
        "id": "qOJgJmrKlzCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\",metrics.accuracy_score(Y_Test, Y_Pred_svm))\n",
        "print(\"Precision:\",metrics.precision_score(Y_Test, Y_Pred_svm, average='micro'))\n",
        "print(\"Recall:\",metrics.recall_score(Y_Test, Y_Test, average='micro'))\n",
        "print(\"F1_Score:\",metrics.f1_score(Y_Test, Y_Test, average='micro'))\n",
        "print(\"Cohen_Kappa_Score:\",sklearn.metrics.cohen_kappa_score(Y_Test, Y_Pred_svm, labels=None, weights=None, sample_weight=None))"
      ],
      "metadata": {
        "id": "7wBkiIhsFHgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Individual Clasification - SVM\n",
        "print(classification_report(Y_Test, Y_Pred_svm))"
      ],
      "metadata": {
        "id": "2NXdnc6aFLjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train DT Classifier"
      ],
      "metadata": {
        "id": "aEe1lRlPl7g2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DT classifier\n",
        "classifier_dt = tree.DecisionTreeClassifier(criterion='gini',\n",
        "                                            splitter='best',\n",
        "                                            random_state=0,\n",
        "                                            max_depth=None)\n",
        "\n",
        "start_time_1 = datetime.now()\n",
        "classifier_dt.fit(X_Train, Y_Train)\n",
        "end_time_1 = datetime.now()\n",
        "print('Training Duration: {}'.format(end_time_1 - start_time_1))\n",
        "\n",
        "#DT Testing\n",
        "start_time_2 = datetime.now()\n",
        "\n",
        "# Predicting the test set results\n",
        "Y_Pred_dt = classifier_dt.predict(X_Test)\n",
        "\n",
        "end_time_2 = datetime.now()\n",
        "print('Testing Duration: {}'.format(end_time_2 - start_time_2))\n",
        "Y_Pred_dt"
      ],
      "metadata": {
        "id": "tUI6yM_Gmcqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DT Confusion Matrix"
      ],
      "metadata": {
        "id": "bD5SAuDilvdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making the Confusion Matrix :DT\n",
        "\"\"\"\n",
        "This creates a heatmap using Seaborn (sn) with the DataFrame df_cm_dt. annot=True adds annotations to each cell,\n",
        "fmt='.0f' formats the annotations as integers,\n",
        "and cmap='Blues' sets the color map to blue shades.\n",
        "\"\"\"\n",
        "\n",
        "cm_dt = confusion_matrix(Y_Test, Y_Pred_dt)\n",
        "cm_dt\n",
        "\n",
        "font = {'family' : 'normal',\n",
        "        'size'   : 14}\n",
        "\n",
        "plt.rc('font', **font)\n",
        "\n",
        "df_cm_dt = pd.DataFrame(cm_dt, index = [i for i in label],\n",
        "                  columns = [i for i in label])\n",
        "\n",
        "plt.figure(figsize = (7,5))\n",
        "sn.heatmap(df_cm_dt, annot=True, fmt='.0f', cmap='Blues')\n",
        "plt.ylabel('True Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SYgb4xRCmffv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DT Evaluation"
      ],
      "metadata": {
        "id": "kSr3zbfhlp1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation for DT\n",
        "\n",
        "print(\"Accuracy:\",metrics.accuracy_score(Y_Test, Y_Pred_dt))\n",
        "print(\"Precision:\",metrics.precision_score(Y_Test, Y_Pred_dt, average='micro'))\n",
        "print(\"Recall:\",metrics.recall_score(Y_Test, Y_Test, average='micro'))\n",
        "print(\"F1_Score:\",metrics.f1_score(Y_Test, Y_Test, average='micro'))\n",
        "print(\"Cohen_Kappa_Score:\",sklearn.metrics.cohen_kappa_score(Y_Test, Y_Pred_dt, labels=None, weights=None, sample_weight=None))\n",
        "#print(\"MSE:\",metrics.mean_squared_error(Y_Test,Y_Pred_svm))"
      ],
      "metadata": {
        "id": "h34zBkUbFXXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ef32cb-c578-4b85-944d-52d5a45eed08"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9996189109820888\n",
            "Precision: 0.9996189109820888\n",
            "Recall: 1.0\n",
            "F1_Score: 1.0\n",
            "Cohen_Kappa_Score: 0.999456690962917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Individual Clasification - DT\n",
        "print(classification_report(Y_Test, Y_Pred_dt))\n",
        "\n",
        "\"\"\"\n",
        "The classification report summarizes these metrics for each class individually and also\n",
        "provides the weighted average (averaged across all classes, weighted by the number of true instances for each class)\n",
        "and the macro average (averaged across all classes,\n",
        "giving each class equal weight regardless of its size) of precision, recall, and F1-score.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3ElaiQ8iFZ3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69ff7f59-5d3a-4b0c-c474-1b8248eb879e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00     18188\n",
            "           2       1.00      1.00      1.00     13251\n",
            "           3       1.00      1.00      1.00      5889\n",
            "           4       1.00      1.00      1.00      7281\n",
            "\n",
            "    accuracy                           1.00     44609\n",
            "   macro avg       1.00      1.00      1.00     44609\n",
            "weighted avg       1.00      1.00      1.00     44609\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting on New Instances with DT\n",
        "Here, we'll make predictions on new data instances or cases."
      ],
      "metadata": {
        "id": "tlXlnxsBmW_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "read_New = pd.read_csv('/content/drive/MyDrive/newdata.csv')[0:10]\n",
        "\n",
        "read_New.iloc[:, 1]= label_encoder.fit_transform(read_New.iloc[:, 1])\n",
        "read_New.iloc[:, 2]= label_encoder.fit_transform(read_New.iloc[:, 2])\n",
        "read_New.iloc[:, 3]= label_encoder.fit_transform(read_New.iloc[:, 3])\n",
        "read_New.iloc[:, 4]= label_encoder.fit_transform(read_New.iloc[:, 4])\n",
        "read_New.iloc[:, 5]= label_encoder.fit_transform(read_New.iloc[:, 5])\n",
        "read_New.iloc[:, 6]= label_encoder.fit_transform(read_New.iloc[:, 6])\n",
        "read_New.iloc[:, 7]= label_encoder.fit_transform(read_New.iloc[:, 7])\n",
        "read_New.iloc[:, 8]= label_encoder.fit_transform(read_New.iloc[:, 8])\n",
        "read_New.iloc[:, 9]= label_encoder.fit_transform(read_New.iloc[:, 9])\n",
        "read_New.iloc[:, 10]= label_encoder.fit_transform(read_New.iloc[:, 10])\n",
        "read_New.iloc[:, 11]= label_encoder.fit_transform(read_New.iloc[:, 11])\n",
        "read_New.iloc[:, 12]= label_encoder.fit_transform(read_New.iloc[:, 12])\n",
        "read_New.iloc[:, 13]= label_encoder.fit_transform(read_New.iloc[:, 13])\n",
        "read_New.iloc[:, 14]= label_encoder.fit_transform(read_New.iloc[:, 14])\n",
        "read_New.iloc[:, 15]= label_encoder.fit_transform(read_New.iloc[:, 15])\n",
        "\n",
        "# Feature Scaling for new instances - though this won't be needed\n",
        "#sc_X = StandardScaler()\n",
        "#X_New = sc_X.transform(read_New)\n",
        "\n",
        "#X_New = []"
      ],
      "metadata": {
        "id": "FciNAAkanSis"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_Pred_single_instance = classifier_dt.predict(read_New)\n",
        "\n",
        "print(\"Prediction for single instance:\", Y_Pred_single_instance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuQRLcLCnNhp",
        "outputId": "cb4da788-f056-4001-ecf3-dda5a5b11f8a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for single instance: [3 3 3 3 3 3 3 3 3 3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN Classifier"
      ],
      "metadata": {
        "id": "eezrFMu2nkSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN classifier\n",
        "classifier_knn = neighbors.KNeighborsClassifier(n_neighbors=5,\n",
        "                                                weights='uniform',\n",
        "                                                algorithm='auto',\n",
        "                                                leaf_size=30,\n",
        "                                                p=2,\n",
        "                                                metric='minkowski',\n",
        "                                                metric_params=None,\n",
        "                                                n_jobs=None)\n",
        "\n",
        "start_time_1 = datetime.now()\n",
        "\n",
        "classifier_knn.fit(X_Train, Y_Train)\n",
        "\n",
        "end_time_1 = datetime.now()\n",
        "print('Training Duration: {}'.format(end_time_1 - start_time_1))\n",
        "\n",
        "# KNN Testing time\n",
        "start_time_2 = datetime.now()\n",
        "\n",
        "# Predicting the test set results\n",
        "Y_Pred_knn = classifier_knn.predict(X_Test)\n",
        "\n",
        "end_time_2 = datetime.now()\n",
        "print('Testing Duration: {}'.format(end_time_2 - start_time_2))"
      ],
      "metadata": {
        "id": "1zmfCdCsFb_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96fb3b7e-0989-4542-e79b-c4c22f1d3449"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Duration: 0:00:00.025143\n",
            "Testing Duration: 0:00:51.094414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN Confusion Matrix"
      ],
      "metadata": {
        "id": "Jbih6m2PnpkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making the Confusion Matrix :knn\n",
        "cm_knn = confusion_matrix(Y_Test, Y_Pred_knn)\n",
        "cm_knn\n",
        "\n",
        "font = {'family' : 'normal',\n",
        "        'size'   : 14}\n",
        "\n",
        "plt.rc('font', **font)\n",
        "\n",
        "df_cm_knn = pd.DataFrame(cm_knn, index = [i for i in label],\n",
        "                  columns = [i for i in label])\n",
        "\n",
        "plt.figure(figsize = (7,5))\n",
        "sn.heatmap(df_cm_knn, annot=True, fmt='.0f', cmap='Blues')\n",
        "plt.ylabel('True Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1toXFc-gFgd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN Evaluation"
      ],
      "metadata": {
        "id": "hkQGtA2rnzEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation for KNN\n",
        "print(\"Accuracy:\",metrics.accuracy_score(Y_Test, Y_Pred_knn))\n",
        "print(\"Precision:\",metrics.precision_score(Y_Test, Y_Pred_knn, average='micro'))\n",
        "print(\"Recall:\",metrics.recall_score(Y_Test, Y_Test, average='micro'))\n",
        "print(\"F1_Score:\",metrics.f1_score(Y_Test, Y_Test, average='micro'))\n",
        "print(\"Cohen_Kappa_Score:\",sklearn.metrics.cohen_kappa_score(Y_Test, Y_Pred_knn, labels=None, weights=None, sample_weight=None))"
      ],
      "metadata": {
        "id": "3OY8IjInFlHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1654cc8b-cda2-4656-c969-ff32af8ae7c4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9776502499495617\n",
            "Precision: 0.9776502499495617\n",
            "Recall: 1.0\n",
            "F1_Score: 1.0\n",
            "Cohen_Kappa_Score: 0.968105350391031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Individual Clasification- KNN\n",
        "print(classification_report(Y_Test, Y_Pred_knn))"
      ],
      "metadata": {
        "id": "CL02WyKeFnp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf977731-2885-4c83-bf04-e8bfe8f3653f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.98      0.99      0.98     18188\n",
            "           2       0.97      0.97      0.97     13251\n",
            "           3       1.00      1.00      1.00      5889\n",
            "           4       0.97      0.96      0.97      7281\n",
            "\n",
            "    accuracy                           0.98     44609\n",
            "   macro avg       0.98      0.98      0.98     44609\n",
            "weighted avg       0.98      0.98      0.98     44609\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting New Instances with KNN"
      ],
      "metadata": {
        "id": "SUrFSBJxn5th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_Pred_single_instance = classifier_knn.predict(read_New)\n",
        "\n",
        "print(\"Prediction for single instance:\", Y_Pred_single_instance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7I53WC__dvw",
        "outputId": "a4d40cf6-bc67-4ef1-f880-382427008f55"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for single instance: [3 3 3 3 3 3 3 3 3 3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:486: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}